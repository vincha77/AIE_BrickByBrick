{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Acknowledgements\n",
        "\n",
        "This notebook was created by Greg Loughnane (\"Dr. Greg\"), Chris Alexiuk (\"The Wiz\") and the team at AI Makerspace.  They walked through this notebook during their excellent Live event on \"RAG for Complex PDFs with LlamaParse and LlamaIndex v0.10\".\n",
        "\n",
        "Link to Youtube video below:\n",
        "https://www.youtube.com/playlist?list=PLrSHiQgy4VjE4W4WTU-wLK7GCpSj_E9Gn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6KblfsJ1Rf-"
      },
      "source": [
        "# LlamaParse - Parsing Complex Documents\n",
        "\n",
        "### NOTE - this notebook is best run on Google Colab with GPU support enabled!\n",
        "\n",
        "With the release of [LlamaParse](https://github.com/run-llama/llama_parse) and [LlamaCloud](https://cloud.llamaindex.ai), LlamaIndex is demonstrating the next step of evolution for its offerings!\n",
        "\n",
        "From the repository:\n",
        "\n",
        "> LlamaParse is an API created by LlamaIndex to efficiently parse and represent files for efficient retrieval and context augmentation using LlamaIndex frameworks.\n",
        "\n",
        "What LlamaIndex has done is created an API Endpoint that we can access (currently for free up to 10,000 pages of PDFs a day) that will parse out PDF files into either plain-text or markdown. That second one means we have a way to retain structural data that can be leveraged for more structural queries!\n",
        "\n",
        "They've also [recently released](https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8) their v0.10 which, similar to LangChain's v0.1.0, provides some stability and methodological changes to move LlamaIndex into the production-ready space. (seeyah later `ServiceContext`!)\n",
        "\n",
        "Let's dive in and see what we can do with this new tool!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9FkgdSN2i1Q"
      },
      "source": [
        "## Load and Parse PDFs\n",
        "\n",
        "We'll start, as always, by grabbing some dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K44OSqKhPrh",
        "outputId": "d353196d-be72-4233-f2b1-972f7cc4c004"
      },
      "outputs": [],
      "source": [
        "!pip install -qU llama-index llama-parse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IxioOtR2pQz"
      },
      "source": [
        "We'll need to provide a LlamaCloud API key to continue.\n",
        "\n",
        "You can find this by following these steps:\n",
        "\n",
        "1. Sign in with one of their many SSO options.\n",
        "  - ![image](https://i.imgur.com/WFH6CPK.png)\n",
        "2. Navigate to the References in the bottom left hand corner of the screen and select `API Key`.\n",
        "  - ![image](https://i.imgur.com/nlw1mo2.png)\n",
        "3. Generate a new key, name it, and keep it in a safe place!\n",
        "  - ![image](https://i.imgur.com/Rxshpeq.png)\n",
        "\n",
        "\n",
        "Now that we have our API Key - let's provide it as an environment variable below.\n",
        "\n",
        "You can also pass the key directly into the `LlamaParse` object we'll create later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BuHZ38MhjzH",
        "outputId": "0785dd20-2c82-40c1-90b8-f9fc5b1b9675"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass.getpass(\"LLamaParse API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU4-fvh73UGq"
      },
      "source": [
        "Since we'll be using OpenAI as our LLM today - we'll need to pass that API key as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMQ_kwt-n5Lf",
        "outputId": "fd7461de-bfb1-47e3-aaf1-5b62061bb518"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQGJFiok3YBE"
      },
      "source": [
        "Let's make sure we can run async in our Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAju5QV-iCNI"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiXGKNd43cXC"
      },
      "source": [
        "### LlamaParse Initialization\n",
        "\n",
        "Here we can initialize our `LlamaParse` object.\n",
        "\n",
        "Notice that there's a few parameters worth paying attention to:\n",
        "\n",
        "- `result_type` - at time of writing this notebook the options are limited to `\"text\"` and `\"markdown\"`. Markdown will be our choice as it will retain structured information quite nicely.\n",
        "- `num_workers` - this will let us set how many workers we'll need. Generally we'll want to set this to the number of files we're going to need to parse. (the maximum is `10`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6qrLAwpiN55"
      },
      "outputs": [],
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    verbose=True,\n",
        "    language=\"en\",\n",
        "    num_workers=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Klt9HvD4B1D"
      },
      "source": [
        "### Uploading Files\n",
        "\n",
        "We'll next need to upload some files to test our the parser!\n",
        "\n",
        "Let's use [NVIDIA's 10-K](https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/1cbe8fe7-e08a-46e3-8dcc-b429fc06c1a4.pdf) and the [Office of Educational Technology's AI and the Future of Learning report](https://www2.ed.gov/documents/ai-report/ai-report.pdf).\n",
        "\n",
        "You can upload them below - be careful to make sure the file matches with what you've uploaded!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "riyF7R_ljPVN",
        "outputId": "4029871d-2c3c-4567-fb68-f70cf11642e1"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "nvidia_earnings_report = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "9mOEv1kelBIU",
        "outputId": "1fb80641-9279-4cf6-fc8c-d68ab37979db"
      },
      "outputs": [],
      "source": [
        "ai_report = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afMigjas4xve"
      },
      "source": [
        "### Parsing Our Files\n",
        "\n",
        "Now that we've uploaded our files and set-up our `LlamaParser` we're ready to parse some files!\n",
        "\n",
        "Running this cell seems very inconsistent - with some files taking ~6min., and others taking ~4s. It seems there is some level of caching, but you can medium -> long wait times for this next cell.\n",
        "\n",
        "> NOTE: As of time of writing, only `.pdf` files are accepted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkJuc1ypj8ar",
        "outputId": "8662b610-bcfd-4114-cf23-4383984175db"
      },
      "outputs": [],
      "source": [
        "documents = parser.load_data([\"./nvidia-earnings.pdf\", \"./ai-report.pdf\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3ZrdM5s5Jd_"
      },
      "source": [
        "Let's look at our 10-K example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHR__3VinSEu",
        "outputId": "9642e716-8fe1-49e3-815c-160c50153511"
      },
      "outputs": [],
      "source": [
        "print(documents[0].text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1BdkNPy5R37"
      },
      "source": [
        "Right away we can see that some kind of structure is being retained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jsw6GsznYxK",
        "outputId": "4d23177f-01ba-4513-cd13-e77b4a6a6e77"
      },
      "outputs": [],
      "source": [
        "print(documents[1].text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4OYuTZt5XTU"
      },
      "source": [
        "The same is true of our AI Education report!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDv5moIApMfA"
      },
      "source": [
        "## LlamaIndex Recursive Query Engine\n",
        "\n",
        "Now that we have some parsed objects - let's see how well we can leverage them using one of the [example query engines](https://github.com/run-llama/llama_parse/blob/main/examples/demo_advanced.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eee3nPrL5mmV"
      },
      "source": [
        "### Setting our...Settings\n",
        "\n",
        "That's right! `ServiceContext` is dead, long live `Settings`.\n",
        "\n",
        "Let's point our generic LLM to `gpt-3.5-turbo` and our generic embedding model as `text-embedding-3-small`.\n",
        "\n",
        "> NOTE: You'll notice we're pulling `Settings` our of `llama_index.core` which is a major part of their `v0.10` update!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRD5vth1pV_v"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hShvdoLK6bmQ"
      },
      "source": [
        "We're going to use a `MarkdownElementNodeParser` to help make sense of our Markdown objects so we can leverage the potentially structured information in the parsed documents.\n",
        "\n",
        "- Check out the [docs](https://docs.llamaindex.ai/en/stable/api/llama_index.core.node_parser.MarkdownElementNodeParser.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK4YTyR-nZl2"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
        "\n",
        "node_parser = MarkdownElementNodeParser(llm=OpenAI(model=\"gpt-3.5-turbo\"), num_workers=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMvADx2I65qa"
      },
      "source": [
        "Let's parse!\n",
        "\n",
        "> NOTE: There appears to be inconsistent errors - but the parser is largely able to extract and understand structured data within the document provided by the parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jdF-BnJoGzD",
        "outputId": "e83cfa86-b517-4032-e120-b8625a263b9a"
      },
      "outputs": [],
      "source": [
        "nodes = node_parser.get_nodes_from_documents(documents=[documents[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZE-gV5d7F1X"
      },
      "source": [
        "Now we can extract our `base_nodes` and `objects` to create our `VectorStoreIndex`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8IadyiYoM4g"
      },
      "outputs": [],
      "source": [
        "base_nodes, objects = node_parser.get_nodes_and_objects(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhIYRTPH9H6M"
      },
      "source": [
        "Let's build the index!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A7H17xlo7Vv"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "recursive_index = VectorStoreIndex(nodes=base_nodes+objects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63zQ-SmM9nu9"
      },
      "source": [
        "### Recursive Query Engine\n",
        "\n",
        "Now we can build our Recursive Query Engine with reranking!\n",
        "\n",
        "We'll need to do a few steps:\n",
        "\n",
        "1. Initalize our reranker using `FlagEmbeddingReranker` powered by the `BAAI/bge-reranker-large`.\n",
        "2. Set up our recursive query engine!\n",
        "\n",
        "First, let's install some requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGhgMxPyqOFS",
        "outputId": "46559688-daa1-4e9d-ffbd-0689f91b6db6"
      },
      "outputs": [],
      "source": [
        "!pip install -qU llama-index-postprocessor-flag-embedding-reranker git+https://github.com/FlagOpen/FlagEmbedding.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEC7Er8o_NOi"
      },
      "source": [
        "First up, we'll initialize our reranker - we'll be leveraging [this](https://github.com/FlagOpen/FlagEmbedding) repo to leverage our [`BAAI/bge-reranker-large`](https://huggingface.co/BAAI/bge-reranker-large).\n",
        "\n",
        "Once that's done - we can follow a fairly standard flow of creating our query engine!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLlEWwpFqFrp",
        "outputId": "76f586fe-8a37-42cc-9a0e-9545aa4ad07f"
      },
      "outputs": [],
      "source": [
        "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
        "\n",
        "reranker = FlagEmbeddingReranker(\n",
        "    top_n=5,\n",
        "    model=\"BAAI/bge-reranker-large\",\n",
        ")\n",
        "\n",
        "recursive_query_engine = recursive_index.as_query_engine(\n",
        "    similarity_top_k=15,\n",
        "    node_postprocessors=[reranker],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9uc7dxr_zx_"
      },
      "source": [
        "## NVIDIA 10-K Test\n",
        "\n",
        "Now we can test this on our documents! Let's start with our 10-K document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atjlGCjmq6y7",
        "outputId": "75b64110-49a3-49fb-910b-29014eb33051"
      },
      "outputs": [],
      "source": [
        "query = \"Who is the E-VP, Operations - and how old are they?\"\n",
        "response = recursive_query_engine.query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R6XBRbTrIeR",
        "outputId": "5b669f4c-e58d-4952-d90e-81093b63d845"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_AYaHdSw-kf"
      },
      "source": [
        "![image](https://i.imgur.com/OZcPlJw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "795ao5uWAIfC"
      },
      "source": [
        "As you can see - this information was retrieved extremely well!\n",
        "\n",
        "> NOTE: The actual response time was in the 2-3min. timeframe for the full query which is likely due to running this instance on CPU - meaning the reranking process was a bottleneck. You may find better performance running this notebook in a GPU enabled instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeJZq58ns6j9",
        "outputId": "e662b834-5b9f-4a06-df7a-ce13a78450dd"
      },
      "outputs": [],
      "source": [
        "query = \"What is the gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023?\"\n",
        "response = recursive_query_engine.query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu4KqhmbtIZ5",
        "outputId": "22f54064-e384-4112-d835-2eb4a09c27fb"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp3GBAQdumLk"
      },
      "source": [
        "![image](https://i.imgur.com/9jwFpWk.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z2vazxzAeyh"
      },
      "source": [
        "Another big win for LlamaParse!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nRyIAwzAvZi"
      },
      "source": [
        "## Testing it on the AI Education Report\n",
        "\n",
        "The results for the 10-K were incredible - but will the AI Education Report hold up?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N30vLMMmunNx",
        "outputId": "4593d7af-0e86-4a94-fd2f-c5784cf73db1"
      },
      "outputs": [],
      "source": [
        "ai_report_nodes = node_parser.get_nodes_from_documents(documents=[documents[1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-sy_bQ8vsgc"
      },
      "outputs": [],
      "source": [
        "ai_base_nodes, ai_objects = node_parser.get_nodes_and_objects(ai_report_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke8_eEvhvrU6"
      },
      "outputs": [],
      "source": [
        "ai_recursive_index = VectorStoreIndex(nodes=ai_base_nodes+ai_objects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGZFBdMUv2g5"
      },
      "outputs": [],
      "source": [
        "reranker = FlagEmbeddingReranker(\n",
        "    top_n=5,\n",
        "    model=\"BAAI/bge-reranker-large\",\n",
        ")\n",
        "\n",
        "ai_recursive_query_engine = ai_recursive_index.as_query_engine(\n",
        "    similarity_top_k=15,\n",
        "    node_postprocessors=[reranker],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQYUVBLkwAu7",
        "outputId": "4f3b9132-8ad1-4b3d-e94a-40c2e0ad7b71"
      },
      "outputs": [],
      "source": [
        "query = \"How many AI publications on pattern recognition was there in 2020?\"\n",
        "response = ai_recursive_query_engine.query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7PyZFGHwa4P",
        "outputId": "725eceee-7737-46a1-fc5a-f785230a1ea2"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_FIOOyBxI3d"
      },
      "source": [
        "![image](https://i.imgur.com/tbGtUX2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EIwtcJcA5mj"
      },
      "source": [
        "While the query engine *did* retrieve context that was literally on the figure - it was not the correct information, in any way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C_f3OfRwR3O",
        "outputId": "1746b1e9-625f-412e-cda9-4dcada6bde72"
      },
      "outputs": [],
      "source": [
        "query = \"Can you describe what Figure 14 is related to?\"\n",
        "response = ai_recursive_query_engine.query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsgT_ohHxHme",
        "outputId": "194a0dde-8f4b-40a5-e642-a2cfe3da8170"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuVC2-YnysWA"
      },
      "source": [
        "![image](https://i.imgur.com/T7nVQj8.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7l5TZFuBB7j"
      },
      "source": [
        "As you can see - the query engine did not successfully retrieve context related to the correct Figure. If you read the report, you'll notice that it found information related to Fig. 13."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
